% Encoding: UTF-8

@Article{Brunton2016,
  author    = {Steven L. Brunton and Joshua L. Proctor and J. Nathan Kutz},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  year      = {2016},
  month     = {mar},
  number    = {15},
  pages     = {3932--3937},
  volume    = {113},
  doi       = {10.1073/pnas.1517384113},
  publisher = {Proceedings of the National Academy of Sciences},
}

@Article{desilva2020,
  author    = {Brian de Silva and Kathleen Champion and Markus Quade and Jean-Christophe Loiseau and J. Kutz and Steven Brunton},
  journal   = {Journal of Open Source Software},
  title     = {PySINDy: A Python package for the sparse identification of nonlinear dynamical systems from data},
  year      = {2020},
  number    = {49},
  pages     = {2104},
  volume    = {5},
  doi       = {10.21105/joss.02104},
  publisher = {The Open Journal},
  url       = {https://doi.org/10.21105/joss.02104},
}

@Article{Rackauckas2020,
  author      = {Christopher Rackauckas and Yingbo Ma and Julius Martensen and Collin Warner and Kirill Zubov and Rohit Supekar and Dominic Skinner and Ali Ramadhan and Alan Edelman},
  title       = {Universal Differential Equations for Scientific Machine Learning},
  abstract    = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring "big data". In this work we develop a new methodology, universal differential equations (UDEs), which augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.},
  date        = {2020-01-13},
  eprint      = {2001.04385},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/2001.04385v3:PDF},
  keywords    = {cs.LG, math.DS, q-bio.QM, stat.ML},
}

@Article{Krishnamoorthy2011,
  author      = {Aravindh Krishnamoorthy and Deepak Menon},
  title       = {Matrix Inversion Using Cholesky Decomposition},
  abstract    = {In this paper we present a method for matrix inversion based on Cholesky decomposition with reduced number of operations by avoiding computation of intermediate results; further, we use fixed point simulations to compare the numerical accuracy of the method.},
  date        = {2011-11-17},
  eprint      = {1111.4144},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1111.4144v2:PDF},
  keywords    = {cs.MS},
}

@Article{Martinsson2015,
  author      = {Per-Gunnar Martinsson and Gregorio Quintana-Orti and Nathan Heavner and Robert van de Geijn},
  title       = {Householder QR Factorization with Randomization for Column Pivoting (HQRRP). FLAME Working Note 78},
  abstract    = {A fundamental problem when adding column pivoting to the Householder QR factorization is that only about half of the computation can be cast in terms of high performing matrix-matrix multiplications, which greatly limits the benefits that can be derived from so-called blocking of algorithms. This paper describes a technique for selecting groups of pivot vectors by means of randomized projections. It is demonstrated that the asymptotic flop count for the proposed method is $2mn^2 - (2/3)n^3$ for an $m\times n$ matrix, identical to that of the best classical unblocked Householder QR factorization algorithm (with or without pivoting). Experiments demonstrate acceleration in speed of close to an order of magnitude relative to the {\sc geqp3} function in LAPACK, when executed on a modern CPU with multiple cores. Further, experiments demonstrate that the quality of the randomized pivot selection strategy is roughly the same as that of classical column pivoting. The described algorithm is made available under Open Source license and can be used with LAPACK or libflame.},
  date        = {2015-12-08},
  eprint      = {1512.02671},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1512.02671v2:PDF},
  keywords    = {math.NA, cs.NA},
}

@Book{Knoll2016,
  author    = {Knoll, Carsten},
  publisher = {Carsten Knoll},
  title     = {Regelungstheoretische Analyse- und Entwurfsansätze für unteraktuierte mechanische Systeme},
  year      = {2016},
  address   = {Dresden},
  isbn      = {9783741858994},
}

@Article{Courrieu2008,
  author       = {Pierre Courrieu},
  title        = {Fast Computation of Moore-Penrose Inverse Matrices},
  abstract     = {Many neural learning algorithms require to solve large least square systems in order to obtain synaptic weights. Moore-Penrose inverse matrices allow for solving such systems, even with rank deficiency, and they provide minimum-norm vectors of synaptic weights, which contribute to the regularization of the input-output mapping. It is thus of interest to develop fast and accurate algorithms for computing Moore-Penrose inverse matrices. In this paper, an algorithm based on a full rank Cholesky factorization is proposed. The resulting pseudoinverse matrices are similar to those provided by other algorithms. However the computation time is substantially shorter, particularly for large systems.},
  date         = {2008-04-30},
  eprint       = {0804.4809},
  eprintclass  = {cs.NE},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/0804.4809v1:PDF},
  journaltitle = {Neural Information Processing - Letters and Reviews 8, 2 (2005) 25-29},
  keywords     = {cs.NE},
}

@Comment{jabref-meta: databaseType:bibtex;}
